<!-- =======================================================
CoF-T2I README (GitHub)
Edits per your request:
1) Logo on the LEFT of the title (project-page style)
2) Keep "CoF-T2I" text as-is (but colored yellow via HTML span)
3) Use emoji for section headers
4) Shrink the "Comparison" figure (width=82%)
======================================================= -->

<div align="center">
  <h1 style="border-bottom: none; margin-bottom: 0;">
      <img src="assets/logo.png" alt="logo" height="48" style="vertical-align: bottom; margin-right: 10px;" />
      <span style="color:#E67E22; font-style:italic; font-weight:900;">CoF-T2I:</span>
  </h1>

  <h2 style="margin:10px 0 0 0;">
    Video Models as Pure Visual Reasoners for Text-to-Image Generation
  </h2>
</div>


<div align="center">
<p style="margin-top:10px;">
  <a href='https://cof-t2i.github.io/'><img src='https://img.shields.io/badge/ğŸ“° Project-Page-blue' height="25"></a>
  <a href="#"><img src="https://img.shields.io/badge/arXiv-arXiv-red?style=badge&logo=arXiv" alt="Paper PDF" height="25"></a>
  <a href='#'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Model-Comming%20Soon-lightgrey' height="25"></a>
  <a href='#'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Coming%20Soon-lightgrey' height="25"></a>
</p>
</div>

 

## âœ¨ News

- **[2026.01.14]** ğŸš€ğŸš€ğŸš€ We have realsed **CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation**, checkout **[** [Paper](); [Website](https://cof-t2i.github.io) **]**.

## ğŸ”ï¸ Todo List

We are actively preparing to release the following:
  - [x] Paper & project page
  - [ ] Training & inference code
  - [ ] CoF-T2I model checkpoints
  - [ ] CoF-Evol-Instruct dataset
  - [ ] Evaluation scripts

## ğŸï¸ CoF-T2I

***CoF-T2I*** brings **Chain-of-Frame (CoF) reasoning** from video generation into text-to-image generation via *progressive visual refinement*:
intermediate frames serve as explicit reasoning steps, and the final frame is taken as the output image.

<p align="center">
  <img src="assets/vis_output.png" width="100%" />
</p>
<p align="center">
  <i>Visualization of the reasoning trajectories generated by CoF-T2I. The final output is shown in large, and intermediate frames are shown in small.</i>
</p>


### ğŸ¯ Contributions
- ğŸ”­ **A novel generation paradigm**: We propose ***CoF-T2I***, a text-to-image model that repurposes a video foundation model as pure visual reasoner, generating images via a CoF reasoning process.

- ğŸ—‚ï¸ **A comprehensive dataset with scalable pipeline**: We introduce ***CoF-Evol-Instruct***, a 64K-scale dataset of progressive visual refinement trajectories, built with a scalable quality-aware pipeline.

- ğŸ“Š **Competitive results with extensive validation**: Our extensive experiments show that CoF-T2I substantially outperforms its video backbone and achieves competitive performance on challenging benchmarks, with additional validations confirming its substantial promise.

### ğŸ” Overview

<p align="center">
  <img src="assets/overview.png" width="100%" />
</p>
<p align="center">
  <i>
    Overview of CoF-T2I. CoF-T2I builds on a video generation backbone, reframing inference-time reasoning for T2I generation as a CoF refinement process. 
  </i>
<p align="center">

## ğŸ“‘ CoF-Evol-Instruct

<p align="center">
  <img src="assets/pipeline.png" width="100%" />
</p>
<p align="center">
<i>We design a quality-aware construction pipeline and curate 64K reasoning trajectories, ensuring both sample-level diversity and frame-wise consistency.</i>
</p>


<!-- ## ğŸ–¼ï¸ Visualizations

### 1) Dataset Visualizations
<p align="center">
  <img src="assets/vis_data.png" width="100%" />
</p>

### 2) Qualitative Comparison
<p align="center">
  <img src="assets/vis_comp.png" width="100%" />
</p>
<p align="center">
  <i>Comparison of the baseline video model (Wan2.1-T2V), Bagel-Think, and CoF-T2I.</i>
</p>

### 3) Reasoning Trajectories
<p align="center">
  <img src="assets/vis_more.png" width="100%" />
</p>

### 4) Step-wise Quality Evolution
<p align="center">
  <img src="assets/evol.png" width="100%" />
</p>
<p align="center">
  <i>Performance trend across reasoning steps on GenEval (left) and Imagine-Bench (right).</i>
</p> -->

## ğŸ«¡ Acknowlegements

We would like to thank the following open-source projects and research works:

- [Diffsynth-Studio](https://github.com/modelscope/DiffSynth-Studio)
- [Draft-as-CoT](https://arxiv.org/pdf/2512.05112)
- [Think-while-Generating](https://think-while-gen.github.io/)
- [Image-Gen-CoT](https://github.com/ZiyuGuo99/Image-Generation-CoT)

## ğŸ§¾ Citation

If you find our work useful, please consider citing:

```bibtex
% TODO: update with arXiv/OpenReview link when available
@misc{cof_t2i_2026,
  title   = {CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation},
  author  = {Tong, Chengzhuo and Chang, Mingkun and Zhang, Shenglong and Wang, Yuran and Liang, Cheng and Zhao, Zhizheng and Zeng, Bohan and Shi, Yang and An, Ruichuan and Zhao, Ziming and Li, Guanbin and Wan, Pengfei and Zhang, Yuanxing and Zhang, Wentao},
  year    = {2026},
  note    = {Project page: https://cof-t2i.github.io/}
}
