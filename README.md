<!-- =======================================================
CoF-T2I README (GitHub)
Edits per your request:
1) Logo on the LEFT of the title (project-page style)
2) Keep "CoF-T2I" text as-is (but colored yellow via HTML span)
3) Use emoji for section headers
4) Shrink the "Comparison" figure (width=82%)
======================================================= -->

<table>
  <tr>
    <td width="140" align="center" valign="middle">
      <!-- put your logo at assets/logo.png -->
      <img src="assets/logo.png" width="110" />
    </td>
    <td valign="middle">
      <h1 style="margin:0; padding:0;">
        <span style="color:#F5C400;">CoF-T2I</span>
      </h1>
      <p style="margin:6px 0 10px 0;">
        <b>Video Models as Pure Visual Reasoners for Text-to-Image Generation</b>
      </p>
      <p style="margin:0;">
        <a href="https://cof-t2i.github.io/"><img src="https://img.shields.io/badge/Project-Page-blue" /></a>
        <a href="#"><img src="https://img.shields.io/badge/Paper-2026.01.14-orange" /></a>
        <a href="https://github.com/VisionChengzhuo/CoF-T2I"><img src="https://img.shields.io/badge/Code-Coming%20Soon-lightgrey" /></a>
        <a href="#"><img src="https://img.shields.io/badge/Dataset-Coming%20Soon-lightgrey" /></a>
      </p>
    </td>
  </tr>
</table>

<p align="center">
  <!-- teaser figure: final frame large + intermediate frames small -->
  <img src="assets/vis_output.png" width="100%" />
</p>
<p align="center">
  <i>Visualization of the reasoning trajectories generated by CoF-T2I. The final output is shown in large, and intermediate frames are shown in small.</i>
</p>

---

## üí• News

- **[2026.01.14]** Paper and project page are released. We are actively preparing to open-source our **code**, **models**, and **dataset**.

---

## üß† CoF-T2I

**CoF-T2I** brings **Chain-of-Frame (CoF) reasoning** from video generation into text-to-image generation via **progressive visual refinement**:
intermediate frames serve as explicit reasoning steps, and the final frame is taken as the output image.

### Abstract
Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference.
With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles).
However, their potential to enhance text-to-image (T2I) generation remains largely under-explored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process.

To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output.
To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that models generation from semantics to aesthetics.
To further improve quality and avoid motion artifacts, we enable independent encoding for each frame.

---

## üß© Comparison: Inference-time Reasoning for T2I

<p align="center">
  <img src="assets/intro.png" width="82%" />
</p>
<p align="center">
  <i>
    Comparison of inference-time reasoning paradigms:
    (a) exterior reward guidance, (b) interleaved textual CoT, (c) our video-based CoF reasoning (CoF-T2I).
  </i>
</p>

---

## üß≠ Overview

<p align="center">
  <img src="assets/overview.png" width="100%" />
</p>

- **Training.** Given a CoF trajectory, we employ a video VAE to **separately encode each frame**, and optimize a vanilla **flow-matching** objective.  
- **Inference.** Starting from noisy initialization, the model denoises to sample a progressively refined reasoning trajectory; **only the final-frame latent is fully decoded** and taken as the output image.  
- **Quality Assessment.** Along the CoF trajectory, text-image alignment and aesthetic quality continue to improve.

---

## üß™ Data Curation: CoF-Evol-Instruct

<p align="center">
  <img src="assets/pipeline.png" width="100%" />
</p>

We design a **quality-aware construction pipeline** to curate reasoning data, ensuring both **sample-level diversity** and **frame-wise consistency**.

---

## üñºÔ∏è Visualizations

### 1) Dataset Visualizations
<p align="center">
  <img src="assets/vis_data.png" width="100%" />
</p>

### 2) Qualitative Comparison
<p align="center">
  <img src="assets/vis_comp.png" width="100%" />
</p>
<p align="center">
  <i>Comparison of the baseline video model (Wan2.1-T2V), Bagel-Think, and CoF-T2I.</i>
</p>

### 3) Reasoning Trajectories
<p align="center">
  <img src="assets/vis_more.png" width="100%" />
</p>

### 4) Step-wise Quality Evolution
<p align="center">
  <img src="assets/evol.png" width="100%" />
</p>
<p align="center">
  <i>Performance trend across reasoning steps on GenEval (left) and Imagine-Bench (right).</i>
</p>

---

## üöß Release Plan (Coming Soon)

We will release:
- training & inference code,
- model checkpoints,
- CoF-Evol-Instruct dataset,
- evaluation scripts.

---

## üßæ Citation

If you find CoF-T2I useful, please consider citing:

```bibtex
% TODO: update with arXiv/OpenReview link when available
@misc{cof_t2i_2026,
  title   = {CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation},
  author  = {Tong, Chengzhuo and Chang, Mingkun and Zhang, Shenglong and Wang, Yuran and Liang, Cheng and Zhao, Zhizheng and Zeng, Bohan and Shi, Yang and An, Ruichuan and Zhao, Ziming and Li, Guanbin and Wan, Pengfei and Zhang, Yuanxing and Zhang, Wentao},
  year    = {2026},
  note    = {Project page: https://cof-t2i.github.io/}
}
